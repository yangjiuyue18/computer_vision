Вопросы к экзамену по дисциплине «Компьютерное зрение»
1. Задачи компьютерного зрения.
2. Постановка задачи обнаружения границ объектов на изображении. Детекторы границ.
3. Детектор границ Кэнни.
4. Преобразование Хафа. Алгоритм обнаружения линий. Алгоритм обнаружения окружностей.
5. Обнаружение особых точек на изображении. Детектор Моравеца.
6. Обнаружение особых точек на изображении. Детектор Харриса. Детектор Shi-Tomasi.
7. Масштабно-инвариантная трансформация признаков (SIFT).
8. Постановка задачи классификации изображений. Линейный классификатор. SVM. Мешок
визуальных слов.
9. Архитектура нейронной сети для классификации изображений.
10. Постановка задачи локализации объекта на изображении. Метрики.
11. Архитектура нейронной сети для локализации объекта на изображении.
12. Постановка задачи обнаружения объектов на изображении. R-CNN.
13. Обнаружения объектов на изображении. Fast R-CNN и Faster R-CNN.
14. Обнаружения объектов на изображении. Single Short Detector.
15. Постановка задачи сегментации изображений. FCN.
16. Архитектура U-Net.
17. Архитектура Mask R-CNN.
18. Метод оценки оптического потока Лукаса-Канаде.
19. Метод оценки оптического потока Horn-Schunck.
20. Нейронные сети для задачи вычисления оптического потока.
21. Задача отслеживания объектов на изображении (Object Tracking).
22. Задача распознавания лиц. Каскады Хаара.
23. Задача распознавания лиц. FaceNet.
24. Постановка задачи поиска изображений. Подходы к её решению.
25. Задача распознавания текста. Подходы к её решению.

## 1. 计算机视觉的任务  Задачи компьютерного зрения

计算机视觉是一门研究如何让计算机通过图像或视频来感知、理解和分析周围环境的学科，其主要任务包括以下几个方面：
1. **图像分类 / классификация изображений / image classification**
   给定一张图片，将其归类为某一类别。例如，识别图片中的内容是猫、狗还是汽车。
2. **目标检测 / обнаружение цели / target detection**  
   在图像中找到特定的目标，并标注其所在的位置（通常用边界框表示）。
3. **图像分割 / сегментация изображения / image segmentation**  
   将图像划分为不同的区域，使每个像素都被分类为特定的类别。分为语义分割（按类别标注像素）和实例分割（分离不同个体）。
4. **目标跟踪 / Целевое отслеживание / object tracking**
   在视频中跟踪一个或多个目标的移动轨迹。
5. **光流估计 / Оценка оптического потока / Optical Flow Estimation**  
   分析图像序列中像素的运动，用于运动分析和视频稳定等任务。
6. **人脸识别 / Распознавание лиц / face recognition**  
   识别图像或视频中的人脸，广泛用于身份验证和监控。
7. **图像生成与增强 / image generation and enhancement**  
   生成新图像或增强现有图像，例如超分辨率、风格迁移和去噪处理。
8. **三维重建 / 3D реконструкция / 3D reconstruction**  
   从二维图像或视频中恢复物体的三维结构。
9. **文本识别（OCR） / text recognition (OCR)**  
   从图像中提取文字信息，用于文档数字化和车牌识别等。
10. **场景理解 / scene understanding**  
    综合分析图像中的多个元素，理解整个场景的语义内容。

计算机视觉的核心在于模仿人类视觉系统，通过算法、模型和工具使机器能够“看懂”图像，并作出相应的反应或决策。这些任务广泛应用于自动驾驶、医疗影像分析、安防监控、虚拟现实等领域。

## 2. 在图像上检测对象边界的问题定义。边缘检测器 Постановка задачи обнаружения границ объектов на изображении. Детекторы границ

边缘检测是计算机视觉中的一项基本任务，旨在从图像中提取出对象的轮廓或边界。边缘通常对应于图像中亮度变化剧烈的区域，是图像中最重要的结构之一。边缘检测的目的是在图像中找到这些突变点，即**图像像素灰度值（Image Pixel Intensity || Интенсивность пикселя изображения）** 急剧变化的地方，以便将对象从背景中分离出来。
### 边缘检测的问题定义
边缘检测的目标是通过分析图像中的灰度变化，识别出图像中具有明显变化的部分，通常这些变化代表着物体的边缘。具体任务可以定义为：  
- 对输入图像进行处理，识别出明显的边缘区域。  
- 输出一个二值化图像，其中边缘区域被标识为白色，其余部分为黑色。
### 边缘检测器 Детекторы границ || Edge Detector
边缘检测器是一种用于执行边缘检测任务的算法或滤波器。常见的边缘检测器包括：
1. **Sobel算子 / Sobel Operator**  
   Sobel算子是一种简单的边缘检测方法，它通过计算图像灰度值的梯度来确定边缘的位置。Sobel算子有两个方向的滤波器，一个用于检测水平边缘，另一个用于检测垂直边缘。通过计算水平和垂直梯度的组合，可以得到边缘强度。
2. **Prewitt算子 / Prewitt Operator**  
   Prewitt算子与Sobel算子相似，也是通过计算图像的梯度来进行边缘检测。其主要区别在于使用的卷积核略有不同，Prewitt算子通常被认为在某些情况下更适合检测较大尺度的边缘。
3. **Canny边缘检测器 / Canny Edge Detector**  
   Canny边缘检测器是最常用的边缘检测方法之一，它包含多个步骤：  
   - **高斯滤波 / Gaussian blur**：用于平滑图像，减少噪声的影响。  
   - **梯度计算 / Gradient Computation**：计算每个像素的梯度方向和幅值。  
   - **非极大值抑制 / Non-Maximum Suppression**：通过抑制非边缘像素来细化边缘。  
   - **双阈值 / Double Threshold**：使用高低两个阈值来确定强边缘、弱边缘和非边缘区域。  
   - **边缘连接 / Edge Linking**：通过连接弱边缘与强边缘来完成最终的边缘检测。
4. **Laplace算子 / Laplace Operator**  
   Laplace算子是一种基于二阶导数的边缘检测方法，它可以检测到图像中的急剧灰度变化。常见的应用包括图像的轮廓提取。
5. **Roberts算子 / Roberts Operator**  
   Roberts算子是一种基于邻域差异的边缘检测方法，它通过计算像素之间的梯度差异来确定边缘的位置，通常用于检测细小的边缘。
6. **Kirsch算子 /  Kirsch Operator**  
   Kirsch算子是一种方向性边缘检测方法，它通过在多个方向上计算图像的梯度值来检测边缘。这种方法通常用于检测具有特定方向的边缘。
### 边缘检测的应用
- **物体识别 / Object Recognition**：通过检测图像中的边缘，可以帮助系统识别出物体的轮廓并进行进一步分析。
- **图像分割 / Image Segmentation**：边缘检测是图像分割的一个重要步骤，可以将图像中的不同区域进行分离。
- **特征提取 / Feature Extraction**：边缘通常包含图像中的重要信息，边缘检测可以帮助提取这些信息，用于后续的图像分析任务。
- **图像增强 / Image Enhancement**：通过边缘检测可以对图像进行增强，突出显示图像中的重要结构。

通过边缘检测，计算机能够从图像中提取出更为清晰、精确的信息，进而完成如目标检测、图像分割等更复杂的任务。

## 3. Canny边缘检测器 Детектор границ Кэнни

Canny边缘检测器Canny Edge Detector是一种多步骤的边缘检测算法，其目标是检测图像中的边缘，同时减少噪声和伪边缘的影响。Canny算法具有以下步骤：  
1. **高斯滤波 / Gaussian Filter**：平滑图像以降低噪声。  
2. **梯度计算 / Gradient Calculation**：计算图像中每个像素点的梯度幅值和方向。  
3. **非极大值抑制 / Non-maximum Suppression**：通过抑制梯度方向上的非最大值像素，精确定位边缘。  
4. **双阈值处理 / Double Thresholding**：使用高低阈值筛选出强边缘和弱边缘。  
5. **边缘连接 / Edge Connection**：通过弱边缘连接强边缘形成最终结果。

### OpenCV伪代码函数  
```python
def canny_edge_detection(image, low_threshold, high_threshold, kernel_size):
    """
    Canny边缘检测伪代码函数

    :param image: 输入图像，通常为灰度图像。
    :param low_threshold: 低阈值，用于筛选弱边缘。
    :param high_threshold: 高阈值，用于筛选强边缘。
    :param kernel_size: 高斯滤波器的核大小，用于平滑图像。
    :return: 检测到边缘的二值图像。
    """
    # Step 1: 高斯滤波去噪
    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)
    # Step 2: Canny边缘检测
    edges = cv2.Canny(blurred_image, low_threshold, high_threshold)
    return edges
```
### 参数解释  
1. **`image`**  
   - 输入图像，通常为灰度图像。  
   - 类型：二维数组。  
2. **`low_threshold`**  
   - 较低的阈值，用于识别弱边缘。如果某个像素的梯度值小于此阈值，将被视为非边缘。  
   - 类型：整数。  
3. **`high_threshold`**  
   - 较高的阈值，用于识别强边缘。如果某个像素的梯度值大于此阈值，将直接视为边缘。  
   - 类型：整数。  
4. **`kernel_size`**  
   - 高斯滤波器的核大小，通常为奇数（如3、5、7）。核越大，图像平滑程度越高，但可能导致细节损失。  
   - 类型：整数。  
5. **返回值**  
   - 一个二值图像，其中白色像素表示边缘，黑色像素表示非边缘。  
   - 类型：二维数组。  

假设我们有一幅灰度图像 `img`：  
```python
edges = canny_edge_detection(img, low_threshold=50, high_threshold=150, kernel_size=3)
cv2.imshow('Canny Edges', edges)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
这样，就可以根据阈值调节边缘检测的灵敏度，最终得到检测到的边缘。

## 4. 霍夫变换。直线检测算法。圆形检测算法 Преобразование Хафа. Алгоритм обнаружения линий. Алгоритм обнаружения окружностей

霍夫变换是一种用来从图像中检测几何形状（如直线、圆形等）的方法。其核心思想是将几何形状的检测问题转化为参数空间的搜索问题。

1. **直线检测 / Line Detection**  
   通过将图像中的点转换到参数空间，寻找所有共线点的参数组合，最终找到直线。  
   参数方程：  
   \[
   \rho = x \cdot \cos\theta + y \cdot \sin\theta
   \]  
   其中：  
   - \(\rho\)：点到原点的距离。  
   - \(\theta\)：点到原点的垂线与x轴的夹角。  

2. **圆形检测 / Circle Detection**  
   圆的参数方程为：  
   \[
   (x - a)^2 + (y - b)^2 = r^2
   \]  
   在霍夫空间中搜索符合圆形方程的点。

### 直线检测 / Line Detection
```python
def hough_line_detection(image, rho, theta, threshold):
    """
    霍夫直线检测伪代码函数

    :param image: 输入图像，通常为边缘检测后的二值图像。
    :param rho: 累积器中距离分辨率，单位为像素。
    :param theta: 累积器中角度分辨率，单位为弧度。
    :param threshold: 最小投票数，表示检测到直线所需的支持点数。
    :return: 检测到的直线的参数 (rho, theta)。
    """
    lines = cv2.HoughLines(image, rho, theta, threshold)
    return lines
```

### 圆形检测 / Circle Detection
```python
def hough_circle_detection(image, dp, min_dist, param1, param2, min_radius, max_radius):
    """
    霍夫圆形检测伪代码函数

    :param image: 输入图像，通常为灰度图像。
    :param dp: 累积器分辨率与输入图像分辨率的反比关系。
    :param min_dist: 检测到的圆之间的最小距离，避免多个重叠圆。
    :param param1: Canny边缘检测的高阈值。
    :param param2: 用于圆心检测的累积器阈值。
    :param min_radius: 检测圆的最小半径。
    :param max_radius: 检测圆的最大半径。
    :return: 检测到的圆的参数 (x, y, r)。
    """
    circles = cv2.HoughCircles(image, cv2.HOUGH_GRADIENT, dp, min_dist, param1=param1, param2=param2, minRadius=min_radius, maxRadius=max_radius)
    return circles
```
## 参数解释  

### 直线检测  
1. **`image`**  
   - 输入二值图像，通常通过边缘检测器（如Canny）预处理。  

2. **`rho`**  
   - 距离分辨率（像素单位），决定霍夫空间的累积精度。  

3. **`theta`**  
   - 角度分辨率（弧度单位），通常为 \(\pi/180\) 或更小。  

4. **`threshold`**  
   - 检测到直线所需的最小累积票数。  

### 圆形检测  
1. **`dp`**  
   - 累积器分辨率与输入图像分辨率的比值，值越大，计算越快，但可能遗漏细节。  

2. **`min_dist`**  
   - 检测到的圆之间的最小距离，避免检测到重叠的圆。  

3. **`param1`**  
   - Canny边缘检测器的高阈值，影响边缘检测结果。  

4. **`param2`**  
   - 用于圆心检测的累积器阈值，值越大，检测的圆越严格。  

5. **`min_radius` 和 `max_radius`**  
   - 圆的最小和最大半径，用于限制检测范围。  

### 直线检测
```python
edges = cv2.Canny(img, 50, 150)
lines = hough_line_detection(edges, rho=1, theta=np.pi/180, threshold=100)
for line in lines:
    rho, theta = line[0]
    a, b = np.cos(theta), np.sin(theta)
    x0, y0 = a * rho, b * rho
    x1, y1 = int(x0 + 1000 * (-b)), int(y0 + 1000 * a)
    x2, y2 = int(x0 - 1000 * (-b)), int(y0 - 1000 * a)
    cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
```

### 圆形检测
```python
circles = hough_circle_detection(gray_img, dp=1, min_dist=20, param1=50, param2=30, min_radius=10, max_radius=50)
for circle in circles[0, :]:
    x, y, r = circle
    cv2.circle(img, (x, y), r, (0, 255, 0), 2)
    cv2.circle(img, (x, y), 2, (255, 0, 0), 3)
```

## 5. 图像上的特征点检测。Moravec检测器 Обнаружение особых точек на изображении. Детектор Моравеца

**Moravec检测器**是最早的**角点(Угол | Corner)** 检测算法之一，用于在图像中识别具有显著特征的点（如角点）。其基本思想是：一个点在多个方向上的灰度变化显著时，可被认为是一个特征点。

Moravec检测器通过计算像素邻域在各个方向上的强度变化，判断某个像素点是否是角点。如果变化超过一定的阈值，则该点被标记为角点。

```python
def moravec_detector(image, window_size, threshold):
    """
    Moravec特征点检测伪代码函数

    :param image: 输入图像，通常为灰度图像。
    :param window_size: 窗口大小，用于计算像素邻域的强度变化。
    :param threshold: 角点强度的最小值，超过此值的点被认为是角点。
    :return: 标记了角点的二值图像。
    """
    # 获取图像尺寸
    rows, cols = image.shape
    # 初始化角点响应矩阵
    response = np.zeros_like(image, dtype=np.float32)

    offset = window_size // 2

    # 遍历图像每个像素
    for y in range(offset, rows - offset):
        for x in range(offset, cols - offset):
            # 初始化最小变化值
            min_variance = float('inf')

            # 遍历四个方向（水平、垂直、两个对角线）
            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                # 计算窗口内的强度差平方和
                variance = 0
                for i in range(-offset, offset + 1):
                    for j in range(-offset, offset + 1):
                        try:
                            diff = image[y + j, x + i] - image[y + j + dy, x + i + dx]
                            variance += diff ** 2
                        except IndexError:
                            pass

                # 更新最小变化值
                min_variance = min(min_variance, variance)

            # 将最小变化值存入响应矩阵
            response[y, x] = min_variance

    # 应用阈值以识别角点
    corners = np.zeros_like(image, dtype=np.uint8)
    corners[response > threshold] = 255

    return corners
```
1. **`image`**  
   - 输入图像，通常为灰度图像。  
   - 类型：二维数组。  
2. **`window_size`**  
   - 用于计算局部变化的窗口大小，通常为奇数（如3、5）。较大的窗口适合检测大尺度特征点。  
   - 类型：整数。  
3. **`threshold`**  
   - 用于判断角点强度的阈值，越大表示更严格的角点选择。  
   - 类型：浮点数或整数。  
4. **返回值**  
   - 一个二值图像，其中白色像素表示检测到的角点，黑色像素表示非角点。  

```python
# 读取灰度图像
gray_img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
# 使用Moravec检测器
corners = moravec_detector(gray_img, window_size=3, threshold=100)
# 可视化结果
cv2.imshow('Moravec Corners', corners)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
### 应用场景  
- **特征匹配**：用于图像拼接、图像对齐。  
- **目标跟踪**：识别图像中稳定的特征点，用于目标的运动分析。  
- **图像分割**：在分割时增强区域边界信息。

## 6. 图像上的特征点检测。Harris检测器。Shi-Tomasi检测器 Обнаружение особых точек на изображении. Детектор Харриса. Детектор Shi-Tomasi

**Harris检测器**是基于图像灰度变化的角点检测算法，其利用**二阶导数和矩阵特征值**来评估一个点是否是角点。它的特点是能够检测到稳定、显著的角点，且对噪声较为鲁棒。  
**Shi-Tomasi检测器**是对Harris检测器的改进版本，它通过直接比较**矩阵的最小特征值**来判断角点的显著性，而非通过组合特征值的响应函数，从而提高了检测精度。

### Harris检测器
```python
def harris_detector(image, block_size, ksize, k, threshold):
    """
    Harris角点检测伪代码函数

    :param image: 输入图像，通常为灰度图像。
    :param block_size: 邻域窗口大小，用于计算角点检测的结构张量矩阵。
    :param ksize: Sobel算子的核大小，用于计算图像梯度。
    :param k: Harris响应公式中的敏感系数，通常在 [0.04, 0.06] 之间。
    :param threshold: 用于角点筛选的响应阈值。
    :return: 标记了角点的二值图像。
    """
    # 计算Harris角点响应
    harris_response = cv2.cornerHarris(image, block_size, ksize, k)
    # 标记角点
    corners = np.zeros_like(image, dtype=np.uint8)
    corners[harris_response > threshold * harris_response.max()] = 255
    return corners
```
### Shi-Tomasi检测器
```python
def shi_tomasi_detector(image, max_corners, quality_level, min_distance):
    """
    Shi-Tomasi角点检测伪代码函数
    :param image: 输入图像，通常为灰度图像。
    :param max_corners: 最大角点数，检测到的角点不会超过此值。
    :param quality_level: 角点质量的最低值，取值范围为 [0, 1]。
    :param min_distance: 角点之间的最小欧几里得距离。
    :return: 角点的坐标列表。
    """
    # 使用Shi-Tomasi检测器
    corners = cv2.goodFeaturesToTrack(image, max_corners, quality_level, min_distance)
    return corners
```
### Harris检测器
1. **`image`**  
   - 输入灰度图像。  
2. **`block_size`**  
   - 计算结构张量矩阵的邻域窗口大小。  
3. **`ksize`**  
   - Sobel算子的核大小，通常为3或5。  
4. **`k`**  
   - Harris响应函数的敏感系数，较小的值更敏感，但可能增加噪声。  
5. **`threshold`**  
   - 用于筛选角点的响应阈值，超过此值的点被认为是角点。  
### Shi-Tomasi检测器
1. **`max_corners`**  
   - 最大角点数，用于限制检测的角点数量。  
2. **`quality_level`**  
   - 角点响应的最低质量因子，较高值会过滤掉弱角点。  
3. **`min_distance`**  
   - 角点之间的最小距离，用于避免角点过于密集。  

### Harris检测器
```python
gray_img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
# Harris角点检测
harris_corners = harris_detector(gray_img, block_size=2, ksize=3, k=0.04, threshold=0.01)
# 可视化结果
cv2.imshow('Harris Corners', harris_corners)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
### Shi-Tomasi检测器
```python
gray_img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
# Shi-Tomasi角点检测
shi_tomasi_corners = shi_tomasi_detector(gray_img, max_corners=100, quality_level=0.01, min_distance=10)
# 可视化角点
for corner in shi_tomasi_corners:
    x, y = corner.ravel()
    cv2.circle(gray_img, (int(x), int(y)), 3, (255, 0, 0), -1)
cv2.imshow('Shi-Tomasi Corners', gray_img)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
### 应用场景  
1. **目标跟踪**：检测关键点以实现跟踪。  
2. **图像拼接**：通过角点匹配对齐图像。  
3. **三维重建**：角点是立体匹配的关键特征。  

## 7. 尺度不变特征变换（SIFT）Scale-Invariant Feature Transform (SIFT) | Масштабно-инвариантное преобразование признаков (SIFT)

**SIFT**（Scale-Invariant Feature Transform）是一种用于检测和描述图像局部特征点的算法，具有**尺度不变性**、**旋转不变性**以及**部分仿射变换不变性**。SIFT通过检测图像中的稳定特征点并为其计算描述符，广泛用于图像匹配、拼接、三维重建、物体识别等计算机视觉任务。
SIFT的核心思想是，在不同的尺度下检测图像的关键点，通过对每个关键点周围的局部区域进行描述，使得即使图像旋转、缩放或改变亮度，仍然能够找到相似的特征点。

```python
def sift_detector(image, nfeatures=0, contrast_threshold=0.04, edge_threshold=10, sigma=1.6):
    """
    SIFT特征点检测伪代码函数

    :param image: 输入图像，通常为灰度图像。
    :param nfeatures: 返回的最大特征点数（默认0表示不限制）。
    :param contrast_threshold: 用于抑制低对比度的特征点的阈值。
    :param edge_threshold: 用于过滤不稳定特征点的阈值。
    :param sigma: 高斯模糊的标准差，用于构建尺度空间。
    :return: 特征点和描述符。
    """
    # 创建SIFT对象
    sift = cv2.SIFT_create(nfeatures, contrastThreshold=contrast_threshold, edgeThreshold=edge_threshold, sigma=sigma)
    # 检测特征点并计算描述符
    keypoints, descriptors = sift.detectAndCompute(image, None)
    return keypoints, descriptors
```
1. **`image`**  
   - 输入灰度图像。  
2. **`nfeatures`**  
   - 需要返回的最大特征点数量。值为0时表示没有上限。  
3. **`contrast_threshold`**  
   - 用于抑制对比度较低的特征点的阈值，较大的值可以去除噪声和不显著的点。  
4. **`edge_threshold`**  
   - 用于剔除边缘响应较弱的特征点的阈值，防止检测到不稳定的点。  
5. **`sigma`**  
   - 用于构建尺度空间的高斯模糊的标准差。  
6. **返回值**  
   - **`keypoints`**：图像中的特征点列表，每个特征点包含其位置、尺度、方向等信息。  
   - **`descriptors`**：每个特征点的描述符，通常是一个向量，用于表示该点的局部特征。

```python
# 读取灰度图像
gray_img = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
# 使用SIFT检测特征点和描述符
keypoints, descriptors = sift_detector(gray_img)
# 在图像中绘制特征点
img_with_keypoints = cv2.drawKeypoints(gray_img, keypoints, None)
# 可视化结果
cv2.imshow('SIFT Keypoints', img_with_keypoints)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
1. **图像匹配**：通过特征点及其描述符匹配不同图像中的相同物体或场景。  
2. **三维重建**：通过匹配不同视角下的特征点来重建三维场景。  
3. **物体识别**：在变化的环境下，基于特征点识别物体。  
4. **图像拼接**：通过特征点匹配来对齐图像，创建全景图像。
SIFT是一个非常强大的特征点检测方法，能处理大量的变化（如缩放、旋转、光照变化等）
## 8. 图像分类任务的定义。线性分类器、支持向量机（SVM）、视觉词袋模型 Постановка задачи классификации изображений. Линейный классификатор. SVM. Мешок визуальных слов  
**图像分类**是计算机视觉中的基础任务之一，其目标是将图像划分到一个或多个预定类别中。为了实现这一目标，通常会通过特征提取方法将图像转化为数值向量，再通过机器学习模型进行分类。常见的图像分类方法包括**支持向量机**（SVM）和**视觉词袋模型**（Bag of Visual Words, BoVW）。
### SVM（支持向量机）
SVM是一种经典的监督学习方法，主要用于二分类和多分类问题。SVM通过寻找一个“最佳超平面”将不同类别的样本进行分离，它具有较强的泛化能力，能够在高维空间中处理复杂的分类任务。对于图像分类，SVM通常结合图像的局部特征（如SIFT、HOG等）来进行分类。
### 视觉词袋模型（BoVW）
视觉词袋模型借鉴了自然语言处理中的词袋模型。它将图像中的特征描述符（如SIFT）进行量化，生成一个固定大小的“视觉词汇”，然后通过这些视觉词汇来表示图像。每个图像被看作是一个“词袋”，而分类则通过这些“视觉单词”的频率分布进行。
### 应用场景
1. **图像分类**  
   图像分类是SVM和视觉词袋模型的经典应用，可以用于将图像分类为不同的类别（如“狗”与“猫”）。
2. **物体识别**  
   使用图像的局部特征进行物体检测和识别，例如在一个场景中识别出“汽车”或“建筑物”。
3. **内容检索**  
   基于图像特征的相似性进行图像检索，即根据用户提供的图像查询数据库中的相似图像。

## 9. 图像分类神经网络的架构 Архитектура нейронной сети для классификации изображений

**图像分类任务中的神经网络架构**旨在通过一系列特定设计的网络层次，自动从图像中提取特征并进行类别预测。架构的设计直接影响分类性能，包括模型的准确性、训练效率和泛化能力。

---
### 1. **LeNet-5**  
最早期的图像分类神经网络，用于手写数字识别。  
- 两个卷积层（提取低级特征，如边缘和角点）。
- 两个平均池化层（减少特征维度）。
- 全连接层用于分类。

### 2. **AlexNet**  
标志深度学习在图像分类领域的崛起，用于ImageNet大规模分类任务。  
- 引入了 **ReLU 激活函数**，提升了非线性表示能力。
- 使用了 **Dropout** 防止过拟合。
- 包含 5 层卷积层和 3 层全连接层。

### 3. **VGG（VGG-16 和 VGG-19）**  
通过更深的网络实现更好的分类性能。  
- 使用多个 3×3 的卷积核堆叠而成，代替大尺寸卷积核。
- 深度为 16 或 19 层，具有明确的层次设计。
- 参数量较大，但分类精度高。

### 4. **Inception（GoogleNet）**  
通过模块化设计，提高了分类性能并减少了参数量。  
- **Inception 模块**：在不同尺度上并行提取特征，包含 1×1、3×3 和 5×5 卷积核。
- 使用 **1×1 卷积核**减少通道数，提高计算效率。
- 模型更轻量化，性能高。

### 5. **ResNet（Residual Network）**  
解决深层网络中的梯度消失问题，实现非常深的网络（如 ResNet-50 和 ResNet-101）。  
- **残差模块（Residual Block）**：通过跳跃连接（skip connection）直接将输入传递到后续层，避免梯度消失。
- 支持超过 100 层的深度设计，同时保持高效训练。

### 6. **DenseNet（Densely Connected Network）**  
进一步优化特征传递和重用，提高训练效率。  
- **密集连接（Dense Connections）**：每一层都与之前的所有层连接，确保特征最大化共享。
- 使用较少的参数实现较高的分类精度。
---
| **架构**      | **深度** | **参数量**       | **主要特点**                       | **适用场景**           |
|---------------|----------|------------------|------------------------------------|------------------------|
| LeNet-5       | 7层       | 很小             | 简单有效，适合小数据集              | 手写数字识别          |
| AlexNet       | 8层       | 中等             | ReLU 和 Dropout 提升性能           | 大型图像分类          |
| VGG           | 16/19层   | 很大             | 多个小卷积核提高特征提取能力        | 高精度图像分类        |
| Inception     | 22层      | 较小             | 并行卷积操作提升计算效率           | 轻量化大数据分类      |
| ResNet        | 50/101层  | 中等             | 跳跃连接解决梯度消失问题           | 深层网络分类任务      |
| DenseNet      | 121层及以上 | 较小             | 最大化特征共享与重用               | 复杂特征分类任务      |

#### 应用  
1. **医学图像分析**：ResNet 和 DenseNet 被广泛应用于医学图像分类，如肿瘤检测和病理学分析。  
2. **自动驾驶**：Inception 和 VGG 在车辆和行人检测中的分类任务表现出色。  
3. **实时图像分类**：Inception 和 MobileNet 等轻量化架构适合实时场景。  
4. **大规模分类**：AlexNet 和 ResNet 在 ImageNet 等大规模分类任务中表现卓越。

## 10. 图像上目标定位问题的定义。评价指标 Постановка задачи локализации объекта на изображении. Метрики.

**物体定位**（Object Localization）是计算机视觉中的一个任务，它的目标是在图像中精确地确定一个或多个物体的位置。通常，定位任务不仅需要识别物体的类别，还要标注其在图像中的位置，通常是通过边界框（bounding box）来表示。
在物体检测任务中，物体定位的精度评估是非常重要的，它直接影响到检测算法的效果。定位精度通常通过不同的**度量标准**来衡量。
### 评估标准
1. **交并比（IoU, Intersection over Union）**  
   **交并比**是用于评估预测边界框与真实边界框重叠度的一个指标。IoU 是通过计算预测框与真实框的交集面积与它们的并集面积之比来得到的。其公式为：
   \[
   IoU = \frac{Area\ of\ Intersection}{Area\ of\ Union}
   \]
   - **值范围**：0 到 1，值越大表示预测框与真实框的重叠程度越高，检测精度越高。
   - **应用**：IoU 用于决定检测结果是否是一个有效的检测，通常会设置一个阈值（如 0.5），如果 IoU 大于该阈值，则认为检测是有效的。
2. **定位精度**  
   定位精度是指预测的边界框与真实边界框之间的位置误差。常见的度量方式有：
   - **中心点误差**：计算预测边界框的中心点与真实边界框中心点之间的距离。
   - **边界框尺寸误差**：比较预测框与真实框的宽度和高度之间的差异。
3. **平均精度（mAP, mean Average Precision）**  
   平均精度是物体检测中常用的评价指标，通常用来评估多个物体类别下的检测性能。mAP 是通过计算每个类别的平均精度（AP），然后对所有类别的 AP 进行平均得到的。AP 的计算方法一般是通过绘制精度-召回曲线并计算其下面积。
   - **公式**：mAP = \(\frac{1}{n} \sum_{i=1}^{n} AP_i\)
     - \(n\)：类别的总数
     - \(AP_i\)：类别 i 的平均精度。
   mAP 越高，说明检测模型在各个类别上的检测效果越好。
4. **召回率与精确率（Recall & Precision）**
   - **精确率（Precision）**：预测为正样本的边界框中，真正为正样本的比例。
     \[
     Precision = \frac{TP}{TP + FP}
     \]
     其中 TP 是真正例，FP 是假正例。
   - **召回率（Recall）**：所有真实正样本中，被正确预测的比例。
     \[
     Recall = \frac{TP}{TP + FN}
     \]
     其中 TP 是真正例，FN 是假负例。
   在定位任务中，精确率和召回率通常是相互制约的，因此需要平衡这两者。
---
1. **自动驾驶**  
   在自动驾驶系统中，物体定位评估非常关键，车辆需要精确定位道路上的行人、车辆、交通标志等物体。高精度的物体定位能帮助自动驾驶系统做出更安全的决策。
2. **视频监控**  
   视频监控系统需要识别并定位场景中的异常物体或行为。定位精度评估可以帮助提升监控系统的有效性和响应速度。
3. **医疗影像**  
   在医学影像分析中，定位精度尤为重要。例如，肿瘤检测需要精确地定位肿瘤在图像中的位置，从而为医生提供准确的诊断信息。
### 总结
物体定位的精度评估是物体检测任务中不可忽视的环节，交并比（IoU）是最常用的定位评估指标。定位精度、mAP、精确率与召回率也是衡量检测精度的重要标准。根据具体应用场景，优化定位精度的评估指标可以大大提升物体检测算法的实际效果和应用价值。 

## 11. 用于目标定位的神经网络架构

物体定位（Object Localization）任务的目标是检测图像中一个或多个物体的位置，并用**边界框（Bounding Box）** 标注出来，同时确定物体的类别。实现这一任务的神经网络架构需要同时预测物体的类别和位置。

在物体定位任务中，通常采用卷积神经网络（CNN）作为基础架构，并在其基础上增加专门设计的模块，用于预测边界框和类别。

### 常见神经网络架构
#### 1. **YOLO（You Only Look Once）**
YOLO 是一种实时物体检测架构，也适用于物体定位。它通过单次前向传播完成图像的全局定位和分类。
- **工作原理**：
  - 将图像分割为 \(S \times S\) 网格。
  - 每个网格预测一定数量的边界框（Bounding Boxes）和其置信度。
  - 同时预测每个边界框中物体的类别。
- **特点**：
  - 快速且实时，适合嵌入式和实时应用。
  - 对小物体定位可能效果欠佳。


#### 2. **Faster R-CNN**
Faster R-CNN 是一种两阶段检测器，专为高精度的物体定位任务设计。
- **工作原理**：
  1. 使用区域提议网络（RPN）生成候选区域（Proposals）。
  2. 使用共享卷积特征对候选区域进行分类和边界框回归。
- **特点**：
  - 精度高，适合复杂场景。
  - 对小物体定位效果较好，但速度相对较慢。

#### 3. **SSD（Single Shot MultiBox Detector）**
SSD 是一种单阶段检测器，通过多个特征层同时检测不同尺度的物体。
- **工作原理**：
  - 使用多个特征层提取不同尺度的特征。
  - 每个特征层直接预测边界框和类别。
- **特点**：
  - 快速高效，适合实时应用。
  - 对小物体的检测能力强于 YOLO，但弱于 Faster R-CNN。

#### 4. **CenterNet**
CenterNet 是一种基于关键点的检测方法，通过检测物体中心点的位置，推断出物体的边界框。
- **工作原理**：
  - 网络预测每个像素点是否是物体中心。
  - 基于中心点的位置信息，预测边界框的宽度和高度。
- **特点**：
  - 更简单，计算成本低。
  - 在小物体检测上表现出色。

### 关键模块
1. **边界框回归（Bounding Box Regression）**  
   神经网络通过回归计算出每个边界框的四个参数：\( (x, y, w, h) \)，分别代表边界框的中心坐标和宽、高。
2. **损失函数**  
   - **分类损失**：用于计算类别预测的误差，常用交叉熵损失（Cross-Entropy Loss）。
   - **定位损失**：用于计算边界框预测与真实框之间的差距，常用 \(L_1\) 或 \(L_2\) 损失。
3. **多尺度特征提取**  
   多尺度特征（如 SSD 中的多特征层）可以提高对不同大小物体的定位精度。
### 应用
1. **自动驾驶**  
   车辆需要定位行人、车辆和障碍物的准确位置，实时检测性能尤为重要。
2. **安防监控**  
   视频监控需要高精度地定位异常行为和可疑物体。
3. **工业检测**  
   在生产线上，定位缺陷产品或物体的具体位置。
### 总结
在物体定位任务中，神经网络架构需要兼顾**定位精度**和**计算效率**。Faster R-CNN 提供高精度，但速度较慢；YOLO 和 SSD 通过单阶段检测实现实时性能。根据具体应用场景，选择合适的架构非常关键。

## 12. 图像目标检测问题的定义。R-CNN

R-CNN（Regions with Convolutional Neural Networks）是一种经典的物体检测方法，其目标是识别图像中的多个物体，同时预测每个物体的边界框和类别。R-CNN 通过结合**区域提议方法**和**卷积神经网络（CNN）**实现物体的精确检测。

### R-CNN 的核心流程
1. **区域提议（Region Proposals）**  
   使用选择性搜索（Selective Search）算法在图像中生成大约 2000 个候选区域。这些区域覆盖了图像中可能包含物体的所有位置。
2. **特征提取（Feature Extraction）**  
   将每个候选区域缩放为固定大小（如 224×224），并使用预训练的CNN（如 AlexNet 或 VGG）提取区域的特征。
3. **分类（Classification）**  
   使用支持向量机（SVM）对提取的特征进行分类，判断该区域是否属于某个目标类别。
4. **边界框回归（Bounding Box Regression）**  
   对预测的边界框进行精细调整，使其更接近真实物体的边界。

#### 优点
- **高检测精度**：通过 CNN 提取特征，大幅提高了检测精度。
- **模块化设计**：特征提取、分类和回归是独立的模块，便于单独优化。
#### 缺点
- **计算效率低**：每个候选区域都需要单独通过 CNN 进行前向传播，导致速度非常慢。
- **存储开销大**：需要存储大量的区域特征，内存占用较高。

### 应用场景
1. **医学影像分析**  
   在 X 光片或 MRI 图像中检测异常区域（如肿瘤）。
2. **自动驾驶**  
   在车载图像中检测其他车辆、行人和交通标志。
3. **遥感图像处理**  
   在卫星图像中定位建筑物、农田或船只。

### 总结  
R-CNN 是物体检测领域的开创性方法，通过结合区域提议和卷积神经网络实现了高精度的物体检测。然而，由于计算效率低，它更适合对实时性要求较低的场景。后续的改进方法（如 Fast R-CNN 和 Faster R-CNN）克服了这一缺点，大幅提高了检测速度和实用性。

## 13. 图像目标检测方法：Fast R-CNN和Faster R-CNN

Fast R-CNN 和 Faster R-CNN 是基于 R-CNN 改进的物体检测方法，旨在提高检测效率和精度。它们通过优化区域提议和特征共享机制，解决了 R-CNN 的**计算效率低**和**存储开销大**的问题。

### Fast R-CNN 的核心改进
1. **共享卷积特征**  
   - Fast R-CNN 对整个输入图像只进行一次卷积操作，生成共享的特征图。
   - 每个候选区域（RoI）通过 RoI 池化层（Region of Interest Pooling）从共享特征图中提取区域特征，而无需重复计算。
2. **RoI 池化层（RoI Pooling）**  
   - 将候选区域映射到共享特征图，并通过池化操作将区域特征变为固定大小的输出。
   - 解决了不同大小的候选区域无法直接输入全连接层的问题。
3. **端到端训练**  
   - Fast R-CNN 将分类和边界框回归整合到同一个神经网络中，可以同时优化这两个任务。
4. **速度提升**  
   - 通过共享特征图和减少冗余计算，Fast R-CNN 相较于 R-CNN 提升了 10 倍以上的检测速度。

### Faster R-CNN 的核心改进
1. **引入区域提议网络（RPN）**  
   - Faster R-CNN 使用 RPN 替代传统的选择性搜索方法，直接从特征图中生成区域提议。
   - RPN 是一个小型神经网络，通过滑动窗口的方式预测每个位置的候选区域。
2. **全卷积特征共享**  
   - RPN 和 Fast R-CNN 共享同一套卷积特征图，进一步提升了计算效率。
3. **端到端训练**  
   - Faster R-CNN 通过联合训练 RPN 和检测网络，使整个流程完全端到端，从而提升了检测精度和训练效率。
---
| **方法**       | **区域提议方式**       | **特征共享**       | **效率**          | **精度**         |
|----------------|------------------------|--------------------|-------------------|------------------|
| **R-CNN**      | 选择性搜索             | 无                 | 慢                | 高               |
| **Fast R-CNN** | 选择性搜索             | 有                 | 快                | 高               |
| **Faster R-CNN** | 区域提议网络（RPN）   | 有                 | 更快              | 更高             |

### Fast R-CNN
**优点**：
- 显著提高了检测速度。
- 保留了较高的检测精度。

**缺点**：
- 区域提议仍依赖选择性搜索，速度受限。
### Faster R-CNN
**优点**：
- RPN 大幅提高了区域提议的速度和质量。
- 完全端到端，整合了提议生成和检测任务。

**缺点**：
- 相比单阶段检测器（如 YOLO、SSD），速度仍然略慢。

### 应用场景
1. **自动驾驶**  
   Faster R-CNN 可用于检测车辆、行人和交通标志，提供高精度的物体检测。
2. **视频监控**  
   适合用于安防领域中监控异常行为或可疑物体的检测。
3. **医学影像**  
   在医学影像中，用于检测肿瘤或病变区域。
---
Fast R-CNN 和 Faster R-CNN 是物体检测领域的重要突破。Fast R-CNN 通过共享特征图和 RoI 池化提高了效率，而 Faster R-CNN 进一步引入 RPN，整合了区域提议与检测任务，成为高精度检测的主流方法。虽然 Faster R-CNN 相较于实时检测方法（如 YOLO）速度略慢，但其精度优势使其适合精度要求较高的场景。

## 14. 图像目标检测方法：单发检测器（Single Shot Detector, SSD）

SSD（Single Shot MultiBox Detector）是一种单阶段物体检测方法，它直接从图像中预测目标的类别和边界框，跳过了传统两阶段检测器（如 Faster R-CNN）的区域提议步骤。SSD 的设计兼顾了检测速度和精度，使其适用于实时检测场景。

### SSD 的核心思想
1. **多尺度特征检测**  
   - SSD 在卷积网络的多个层次上进行预测，这些特征层具有不同的**感受野(Receptive Field)**，能够同时检测大物体和小物体。
2. **卷积特征共享**  
   - 通过共享特征图，SSD 避免了重复计算，提高了检测效率。
3. **直接预测边界框和类别**  
   - SSD 在每个特征图位置直接预测边界框的坐标和对应的物体类别，无需额外的分类器或区域提议网络。
4. **默认框（Default Boxes）**  
   - SSD 为每个特征图位置预定义了一组不同尺度和宽高比的默认框（Anchors），这些框与真实框进行匹配，并通过回归调整预测框。

### SSD 的工作流程
1. **输入图像**  
   将输入图像通过基础卷积网络（如 VGG16 或 MobileNet），提取高维特征图。
2. **多尺度特征层**  
   从不同大小的特征层生成边界框预测：
   - 较浅的层负责检测小物体。
   - 较深的层负责检测大物体。
3. **分类和回归**  
   每个特征图位置直接预测：
   - **类别概率**：物体属于各类的概率。
   - **边界框坐标**：预测框相对于默认框的偏移。
4. **非极大值抑制（NMS）**  
   对多余的边界框进行抑制，仅保留具有最高置信度的框。

#### 优点
1. **高效**：通过单阶段检测，避免了区域提议和分类的分离，速度比两阶段方法更快。
2. **实时性能**：SSD 在标准硬件（如 GPU）上可以实现实时检测。
3. **多尺度检测**：利用多特征层进行预测，对大物体和小物体均有较好的检测效果。
#### 缺点
1. **对小物体不够敏感**：虽然多尺度设计改进了小物体检测，但精度仍然不及 Faster R-CNN。
2. **复杂背景中的性能下降**：在背景复杂的图像中，可能存在误检或漏检。

### 应用场景
1. **实时场景分析**  
   在自动驾驶、视频监控和机器人视觉中，SSD 可实现实时检测和响应。
2. **轻量级设备检测**  
   在资源受限的设备（如嵌入式设备、手机）上，SSD 提供了快速、高效的物体检测方案。
3. **智能家居**  
   用于家用摄像头中的实时物体检测，如识别快递包裹或入侵者。
---
SSD 是一种高效的单阶段检测方法，结合了多尺度特征检测和快速推理，能够在保持高检测精度的同时实现实时性能。相比于两阶段方法（如 Faster R-CNN），SSD 在速度上具有显著优势，但在小物体检测和复杂场景中仍有提升空间。它的设计理念为后续检测器（如 YOLOv3、YOLOv4）提供了重要启发。

## 15. 图像分割问题的定义。全卷积网络（FCN）

全卷积网络（Fully Convolutional Network, FCN）是一种用于**图像分割**的神经网络架构。与传统的卷积神经网络（CNN）不同，FCN 将网络的最后几层全连接层替换为卷积层，从而实现对输入图像中每个像素的分类。FCN 是语义分割的基础方法。

### FCN 的核心思想
1. **全卷积结构**  
   - FCN 中没有全连接层，所有层都是卷积层（包括普通卷积和反卷积）。
   - 全卷积结构允许网络接受任意大小的输入图像，而输出的特征图与输入保持对应关系。
2. **像素级分类**  
   - FCN 的目标是为图像中的每个像素分配一个类别标签，这不同于图像分类任务只输出单个类别。
3. **上采样（Upsampling）**  
   - 使用反卷积（Transpose Convolution）操作将特征图的空间分辨率恢复到原始输入图像的大小。
   - 上采样操作使得输出与输入图像的每个像素一一对应。
4. **跳跃连接（Skip Connection）**  
   - FCN 引入了跳跃连接机制，将浅层特征与深层特征结合，使分割结果既包含高层语义信息，又保留了低层的空间细节。

### FCN 的架构
1. **下采样阶段（Downsampling）**  
   - 使用普通卷积和池化层提取图像特征，同时逐步降低特征图的空间分辨率。
2. **上采样阶段（Upsampling）**  
   - 使用反卷积层将低分辨率的特征图逐步上采样，恢复到输入图像的大小。
3. **跳跃连接**  
   - 将下采样阶段的中间特征图与上采样阶段的特征图融合，增强分割的空间精度。

#### 优点
1. **端到端训练**  
   - FCN 支持从原始输入图像到像素级分类结果的端到端训练，简化了分割流程。
2. **像素级预测**  
   - 直接对每个像素进行分类，分割结果更加细致。
3. **灵活输入大小**  
   - 由于全卷积结构，FCN 可以处理任意大小的输入图像。
#### 缺点
1. **细节丢失**  
   - 下采样阶段会导致空间信息丢失，影响分割边界的精确性。
2. **高计算成本**  
   - 大尺寸图像的分割计算开销较高，尤其是在高分辨率输入时。

### 应用场景
1. **医学图像分割**  
   - 分割病变区域（如肿瘤或器官），辅助医生进行诊断。
2. **遥感图像分析**  
   - 从卫星图像中提取地形、建筑物或植被区域。
3. **自动驾驶**  
   - 分割道路、车辆、行人和其他交通要素，为自动驾驶系统提供场景理解。
4. **场景解析**  
   - 分割自然图像中的不同物体，用于增强现实（AR）或虚拟现实（VR）。
---
全卷积网络（FCN）通过全卷积结构实现了像素级分类，是图像分割领域的基础架构之一。它的跳跃连接和上采样机制在分割任务中具有重要作用。尽管 FCN 在处理细节上存在不足，但它为后续的分割方法（如 U-Net、DeepLab 等）奠定了重要的理论和技术基础。

## 16. U-Net架构。  
## 17. Mask R-CNN架构。  
## 18. Lucas-Kanade光流估计方法。  
## 19. Horn-Schunck光流估计方法。  
## 20. 计算光流任务的神经网络。  
## 21. 图像中的目标跟踪任务（Object Tracking）。  
## 22. 人脸识别任务。Haar级联分类器。  
## 23. 人脸识别任务。FaceNet方法。  
## 24. 图像检索任务的定义及其解决方法。  
## 25. 文本识别任务及其解决方法。  